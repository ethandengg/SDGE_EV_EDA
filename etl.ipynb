{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl.ipynb\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for resource ID: d599c3d3-87af-4e8c-8694-9c01f49e3d93\n",
      "Fetching data for resource ID: 9aa5b4c5-252c-4d68-b1be-ffe19a2f1d26\n",
      "Fetching data for resource ID: 1856386b-a196-4e7c-be81-44174e29ad50\n",
      "Fetching data for resource ID: 888bbb6c-09b4-469c-82e6-1b2a47439736\n",
      "Fetching data for resource ID: 4254a06d-9937-4083-9441-65597dd267e8\n",
      "Fetching data for resource ID: d304108a-06c1-462f-a144-981dd0109900\n",
      "Data has been saved to DMV_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# This chunk gets all the DMV registration data from 2019-2024 and condenses it down into \n",
    "# dictionary to store each data frame\n",
    "\n",
    "# Function to fetch a chunk of data for a given resource ID\n",
    "def fetch_data(resource_id, offset, chunk_size):\n",
    "    url = f\"https://data.ca.gov/api/3/action/datastore_search?resource_id={resource_id}&limit={chunk_size}&offset={offset}\"\n",
    "    with urllib.request.urlopen(url) as fileobj:\n",
    "        response_dict = json.loads(fileobj.read())\n",
    "        return response_dict['result']['records']\n",
    "\n",
    "# Mapping each resource_id to the corresponding year\n",
    "resource_year_map = {\n",
    "    \"d599c3d3-87af-4e8c-8694-9c01f49e3d93\": 2024,\n",
    "    \"9aa5b4c5-252c-4d68-b1be-ffe19a2f1d26\": 2023,\n",
    "    \"1856386b-a196-4e7c-be81-44174e29ad50\": 2022,\n",
    "    \"888bbb6c-09b4-469c-82e6-1b2a47439736\": 2021,\n",
    "    \"4254a06d-9937-4083-9441-65597dd267e8\": 2020,\n",
    "    \"d304108a-06c1-462f-a144-981dd0109900\": 2019\n",
    "}\n",
    "\n",
    "# Function to retrieve all data for a given resource ID\n",
    "def retrieve_data_for_resource(resource_id, chunk_size=50000, total_records=1000000, max_workers=10):\n",
    "    offsets = list(range(0, total_records, chunk_size))\n",
    "    records = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(lambda offset: fetch_data(resource_id, offset, chunk_size), offsets)\n",
    "        for result in results:\n",
    "            records.extend(result)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Stardardize ZIP Code column\n",
    "    if 'Zip Code' in df.columns:\n",
    "        df.rename(columns={'Zip Code': 'ZIP Code'}, inplace=True)\n",
    "    \n",
    "    # Set all dates to the correct year based on resource_id\n",
    "    year = resource_year_map[resource_id]\n",
    "    df['Date'] = year  # Replace entire 'date' column with the year\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of resource IDs for the datasets you want to fetch\n",
    "resource_ids = list(resource_year_map.keys())\n",
    "\n",
    "# Dictionary to store DataFrames for each dataset\n",
    "DMV_dfs = {}\n",
    "\n",
    "# Loop through each resource ID and retrieve data\n",
    "for resource_id in resource_ids:\n",
    "    print(f\"Fetching data for resource ID: {resource_id}\")\n",
    "    DMV_dfs[resource_id] = retrieve_data_for_resource(resource_id)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "dmv_data = pd.concat(DMV_dfs.values(), ignore_index=True)\n",
    "\n",
    "# Save the data to a CSV file (or you could use pickle for smaller file sizes)\n",
    "dmv_data.to_csv('data/DMV_data.csv', index=False)\n",
    "print(\"Data has been saved to DMV_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuel stations data has been saved to fuel_stations.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Fetch AFDC Alternative Fuel Stations Data\n",
    "\n",
    "# Define the AFDC API\n",
    "api_key = 'vterAq8ej7S51meT8RE1FFo5bxFwpHPEpR4Y5GxZ'\n",
    "url = f'https://developer.nrel.gov/api/alt-fuel-stations/v1.json?&api_key={api_key}'\n",
    "\n",
    "# Request data from the AFDC API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    fuel_stations = data['fuel_stations']\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    afdc_data = pd.json_normalize(fuel_stations)\n",
    "\n",
    "    # Save the AFDC data to a CSV file\n",
    "    afdc_data.to_csv('data/fuel_stations.csv', index=False)\n",
    "    print('Fuel stations data has been saved to fuel_stations.csv')\n",
    "else:\n",
    "    print(f'There was an error fetching AFDC data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
